# pylint: skip-file

""""
Title: Utils.py
Author: Han Tong
Date: 2025-07-01
Python Version: Python 3.11.3
Description: All useful functions we use
"""

import logging
import re
from collections import OrderedDict
from itertools import islice
from config import get_config
from scipy.stats import spearmanr
import os
import numpy as np
import pandas as pd
import itertools
import torch
import csv
import warnings
from scipy.linalg import svd
import torch.nn.functional as F
warnings.filterwarnings("ignore")
import time
import logging

from openai import OpenAI 
import matplotlib.pyplot as plt
from torch_geometric.utils import to_undirected
from torch_geometric.utils import add_self_loops
from scipy.stats import spearmanr
import itertools
import io
import sys


config = get_config()
    
def logging_config(config, start_time,
                   level=logging.INFO,
                   console_level=logging.INFO,
                   no_console=True):
    folder = f"{config['path']}/output/{start_time}/"
    if not os.path.exists(folder):
        os.makedirs(folder)
    for handler in logging.root.handlers:
        logging.root.removeHandler(handler)
    logging.root.handlers = []
    logpath = os.path.join(folder, start_time + ".log")
    print("All logs will be saved to %s" %logpath)

    logging.root.setLevel(level)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    logfile = logging.FileHandler(logpath)
    logfile.setLevel(level)
    logfile.setFormatter(formatter)
    logging.root.addHandler(logfile)

    if not no_console:
        logconsole = logging.StreamHandler()
        logconsole.setLevel(console_level)
        logconsole.setFormatter(formatter)
        logging.root.addHandler(logconsole)
    return folder


def get_values(data):
    if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series):
        return data.values
    return data


def grep_index(pattern, name_all):
    """
    find pattern in name_all
    name_all can be a string or strings
    """
    if isinstance(name_all, str):
        return [0] if re.match(pattern, name_all) else []
    else:
        return np.where([re.match(pattern, name) for name in name_all])[0]

def grepl(pattern, name_all):
    """
    whether can we find pattern in name_all or not
    name_all is a string
    """
    return bool(re.match(pattern, name_all))


def id_map(codes, name_all):
    """
    return the location of codes in name_all
    add shift alternatively
    """
    result = [np.where(name_all == name)[0] for name in codes]
    if len(result)==0:
        return np.array([])
    return np.concatenate(result)

        
def match(a, b, rm_None=True):
    # Create an array to store the indices
    indices = np.array([np.where(b == x)[0][0] if x in b else np.nan for x in a])
    # Filter out None values, which represent elements not found in b
    indices = indices[~np.isnan(indices)]
    return indices


def unique_slice(s, num_codes=20):
    """
    return the first num_codes unique codes
    """
    return list(islice(OrderedDict.fromkeys(s), num_codes))
    

def ret_type(codes, sep = ":"):
    """
    codes should be a list []
    """
    codes_split = [code.split(sep)[0] for code in codes]
    return(codes_split)


def find_one_one(code, now_index, name_all):
    """
    help codes find the index of one-one mapping codes across institutions except for leaf LOINC codes
    """
    return np.setdiff1d(id_map([code], name_all), now_index)


def find_other_local(name, P_OTOL, name_all):
    """
    help other lab and local lab to find their corresponding LOINCs generated by asking GPT4 again
    """
    ans = np.array(P_OTOL.iloc[id_map([name], P_OTOL.iloc[:, 0].values), 1 + np.where(~P_OTOL.iloc[id_map([name], P_OTOL.iloc[:, 0].values), 1:].isnull())[1]]).ravel()
    ind = id_map(ans, name_all)
    return ind
    

def find_same_par_gra(hie_loinc_rxn_phe, name_all, name_id, PARENT=1):
    """
    help LOINC, RXNORM, PheCode find their siblings
    hie_loinc_rxn_phe has 3 cols. self, parent, grandpa
    if PARENT = 1, find siblings; is PARENT = 2, find  cousins and siblings
    """

    
    name = name_all[name_id]
    parent_values = hie_loinc_rxn_phe.iloc[:, PARENT].values
    
    name_map = id_map([name], hie_loinc_rxn_phe.iloc[:, 0].values)
    
    if len(name_map)==0:
        return []

    now_id = name_map[0]
    sib_me_id_in_hie = np.where(parent_values == parent_values[now_id])[0]
    
    sib_id_in_hie = list(set(sib_me_id_in_hie) - {now_id})
    sib_names = hie_loinc_rxn_phe.iloc[sib_id_in_hie, 0].values
    
    sib_id_in_name = id_map(sib_names, name_all)
   
    return list(set(sib_id_in_name))

    
    
def sample(codes, max_len = 10):
    """
    sample max_len codes from codes
    """
    if len(codes) < max_len:
        return codes
    else:
        if isinstance(codes, np.ndarray):
            codes = codes.tolist()
        elif isinstance(codes, (set, dict)):
            codes = sorted(codes)
        return codes
    

def sample_cols(data, p):
    """
    sample p fraction columns from data
    """
    num_columns = data.shape[1]
    sampled_columns = np.random.choice(num_columns, int(p * num_columns), replace=False)
    sampled_data = data[:, sampled_columns]
    return sampled_data
    
    
def nega_sample(set1, set2, max_len = 10, DIFF=True):
    """
    Negative sampling function, you can negative sample set1 from set2\set1(if DIFF)
    """
    num_samples = len(set1)
    if DIFF:
        sample_range = set2 - set1
    else:
        sample_range = set2
    return set(np.random.choice(list(sample_range), min(num_samples, len(sample_range), max_len), replace=False))
    
    
def find_rel(name_id, REL_pairs, name_all):
    """
    find related pairs of code in REL_pairs table
    """
    REL_pairs.index = range(REL_pairs.shape[0])
    name = name_all[name_id]
    uni_rel = np.unique(np.union1d(REL_pairs["code2"][id_map([name], REL_pairs["code1"])], REL_pairs["code1"][id_map([name], REL_pairs["code2"])]))
    if len(uni_rel) == 0:
        return set()
    index_now = id_map(uni_rel, name_all)
    return index_now
    

def find_same_type(name_id, name_all):
    """
    find codes from same type
    """
    name = name_all[name_id]
    type_now = np.array(ret_type([name])[0])  # Extract only the first element of the inner list
    type_now = np.reshape(type_now, -1)
    index = id_map([type_now], ret_type(name_all))  # Pass a single-element list containing type_now to id_map
    return index

    
def unique_func(row, size=20):
    """
    return unique codes in row
    keep the order of codes
    """
    unique_elements = pd.unique(row)
    unique_elements = unique_elements[:size]
    return unique_elements


def now_time():
    """
    return time now
    """
    current_time = time.localtime()
    formatted_time = time.strftime("%Y-%m-%d %H:%M:%S", current_time)
    return formatted_time



def save_hyperparameters(config, start_time):
    """
    save hyperparameters in config
    """
    with open(f"{config['path']}/output/{start_time}/hyper_par.csv", "a", newline="") as csvfile:
        fieldnames = list(config.keys())
        writer = csv.writer(csvfile)
        writer.writerow(fieldnames)
        writer.writerow(config.values())

        
def write_file_sub(MGB_AUC1, name, start_time):
    """
    write a AUC row in a file
    """
    column_names = "N"
    row_names = MGB_AUC1.index
    header = [None] * (len(row_names) * 2 + 2)
    header[2::2] = row_names
    header[3::2] = column_names * len(row_names)
    header[0] = "EPOCH"
    header[1] = "BATCH"
    with open(f"{config['path']}/output/{start_time}/{name}.csv", "a") as aucfile:
        aucwriter = csv.writer(aucfile)
        aucwriter.writerow(header)
        aucfile.close()

def write_file_sub2(MGB_AUC1, name, EPOCH, BATCH, start_time):
    """
    change the shape of a row so that it can bewritten into a file
    """
    reshaped_df = MGB_AUC1.T.unstack().to_frame().T
    reshaped_df.insert(0, "Batch", BATCH)
    reshaped_df.insert(0, "Epoch", EPOCH) 
    reshaped_df.to_csv(f"{config['path']}/output/{start_time}/{name}.csv", mode="a", index=False, header=False)



def write_file(Epoch, Batch, config, start_time, loss=None, pre=None, SIM_AUC=None, REL_AUC=None, DRUG_AUC=None):
    """
    write all files (loss pre accuracy and AUC)
    """
    begin = not os.path.exists(f"{config['path']}/output/{start_time}")
    if begin:
        os.mkdir(f"{config['path']}/output/{start_time}") 
        # loss 
        lossfile =  open(f"{config['path']}/output/{start_time}/ALL_LOSS.csv", "a")
        losswriter = csv.writer(lossfile)
    
        if config['path_origin'] == 'align_NA':
            losswriter.writerow(["EPOCH", "BATCH", "align_loss"])     
        
        elif config['path_origin'] is None:    
            losswriter.writerow(["EPOCH", "BATCH", "P_LOSS_hie", "N_LOSS_hie", "P_LOSS_OTOL", "N_LOSS_OTOL", "P_LOSS_SIM_NO_HIE", "N_LOSS_SIM_NO_HIE"])           
        
        else:
            losswriter.writerow(["EPOCH", "BATCH", "P_REL", "N_REL", "P_sppmi", "N_sppmi"])           
        lossfile.close()

        if REL_AUC is not None:
            write_file_sub(REL_AUC, "REL_AUC", start_time)
        
        if SIM_AUC is not None:
            write_file_sub(SIM_AUC, "SIM_AUC", start_time)
            
        if DRUG_AUC is not None:            
            write_file_sub(DRUG_AUC, "DRUG_SIDE", start_time)
            
        if pre is not None:
            prefile = open(f"{config['path']}/output/{start_time}/ALL_PRE.csv", "a")
            prewriter = csv.writer(prefile)
            prewriter.writerow(["EPOCH", "BATCH", "TOP1", "TOP5", "TOP10", "TOP20"])
            prefile.close()
   
        # tuning parameters
        save_hyperparameters(config, start_time)
        
    if loss is not None:
        lossfile = open(f"{config['path']}/output/{start_time}/ALL_LOSS.csv", "a", newline="")
        losswriter = csv.writer(lossfile)
        loss_row = [None] * (len(loss) + 2)
        loss_row[0] = Epoch
        loss_row[1] = Batch
        loss_row[2:] = loss
        losswriter.writerow(loss_row)  

    if pre is not None:
        prefile = open(f"{config['path']}/output/{start_time}/ALL_PRE.csv", "a")
        prewriter = csv.writer(prefile)
        pre_row = [None] * 6
        pre_row[0] = Epoch
        pre_row[1] = Batch
        pre_row[2:] = pre
        prewriter.writerow(pre_row)

    if REL_AUC is not None:
        write_file_sub2(REL_AUC, "REL_AUC", Epoch, Batch, start_time)
    if SIM_AUC is not None:
        write_file_sub2(SIM_AUC, "SIM_AUC", Epoch, Batch, start_time)
    if DRUG_AUC is not None:
        write_file_sub2(DRUG_AUC, "DRUG_SIDE", Epoch, Batch, start_time)
        

def mask_inst(pairs, dict_MGB, shift):
    
    mask = pairs["code1"].isin(dict_MGB) & pairs["code2"].isin(dict_MGB)
    pairs_MGB = pairs[mask]
    pairs_MGB.index = range(pairs_MGB.shape[0])
    temp = np.array(id_map(pairs_MGB["code1"], dict_MGB))
    temp2 = np.array(id_map(pairs_MGB["code2"], dict_MGB))
    pairs_MGB["Var1"] = temp + shift
    pairs_MGB["Var2"] = temp2 + shift
    return(pairs_MGB)

def split_train_set(unique_name, REL_pairs, scale=[0.5, 0.3], seed=42):
    # Set a seed for reproducibility
    random_state = seed
    
    # split train pairs
    num_rows = int(scale[0] * len(REL_pairs)) # add scale ratio of pairs into adj matrix (as train set)
    train_pairs = REL_pairs.sample(n=num_rows, random_state=random_state)
    remaining = REL_pairs[~REL_pairs.index.isin(train_pairs.index)]

    # Split remaining data into validation and test sets
    num_validation = int(scale[1] * len(REL_pairs))
    val_pairs = remaining.sample(n=num_validation, random_state=random_state)
    test_pairs = remaining[~remaining.index.isin(val_pairs.index)]
    
    return train_pairs, val_pairs, test_pairs
    

def remove_duplicate_edge(edge_index):
    edge_index = edge_index.t()
    edge_index =torch.sort(edge_index, dim=1)[0]
    unique_edge_index, inverse_indices = torch.unique(edge_index, return_inverse=True, dim=0)
    unique_edge_index = unique_edge_index.t()
    return unique_edge_index


def get_index(train_rel_pairs, name_all):
    # Convert to set for faster membership checking
    code1_set = set(train_rel_pairs["code1"].tolist())
    code2_set = set(train_rel_pairs["code2"].tolist())

    # Ensure name_all is a list for proper membership checking
    if isinstance(name_all, pd.Series):
        name_all = name_all.tolist()
    elif isinstance(name_all, np.ndarray):
        name_all = name_all.tolist()
    # else assume it"s already a list

    rel_index_1 = np.where([name in code1_set for name in name_all])[0]
    rel_index_2 = np.where([name in code2_set for name in name_all])[0]
    rel_index = np.unique(np.union1d(rel_index_1, rel_index_2))

    return rel_index

def process_predictions(Pre, LEVEL, other_name, item_dict):
    n = len(Pre)
    right_top1 = np.zeros(n)
    right_top5 = np.zeros(n)
    right_top10 = np.zeros(n)
    right_top20 = np.zeros(n)
    ans = 0
    for i in range(n):
        predict_top1 = Pre[i, 0]
        predict_top5 = Pre[i, 0:5]
        predict_top10 = Pre[i, :10]
        predict_top20 = Pre[i, :]
        
        ans += 1
        level_data = item_dict[other_name[i]]

        right_top1[i] = int(predict_top1 in level_data)
        right_top5[i] = int(any(np.isin(predict_top5, level_data)))
        right_top10[i] = int(any(np.isin(predict_top10, level_data)))
        right_top20[i] = int(any(np.isin(predict_top20, level_data)))

    return right_top1, right_top5, right_top10, right_top20, ans

def solve_Procrustes(X1, X2):
    """
    \Omega = argmin ||X1 - X2\Omega||_F
    X1: local code
    X2: LOINC code sapbert emb

    In this function, we can get the best \Omega, so that we can get the new Loinc and the new TOP20
    """
    temp = X2.T @ X1
    u, s, vt = svd(temp)
    return u @ vt


        
def my_item(x):
    if x is None:
        return None
    elif not isinstance(x, torch.Tensor):
         x = torch.tensor(x)
    return x.item()


def weight_auc(RELA_MGB_AUC):
    return sum(RELA_MGB_AUC[0].num.values[~np.isnan(RELA_MGB_AUC[0].auc.values)] * RELA_MGB_AUC[0].auc.values[~np.isnan(RELA_MGB_AUC[0].auc.values)])/sum(RELA_MGB_AUC[0].num.values[~np.isnan(RELA_MGB_AUC[0].auc.values)])


def func_type(RELA_MGB_AUC):
    return RELA_MGB_AUC[0].iloc[~np.isnan(RELA_MGB_AUC[0].auc.values),]


def sum_(SIMI_MGB):
    return(sum(func_type(SIMI_MGB).iloc[:,0].values *  func_type(SIMI_MGB).iloc[:,1].values)/sum(func_type(SIMI_MGB).iloc[:,1].values))



def my_diff(cos1, true_rank):
    ans = [np.mean(compute_spearman(cos1[i,:], true_rank)) for i in range(cos1.shape[0])]
    return ans



def combine_2_array(array1, array2):

    array1 = torch.tensor(array1)
    array2 = torch.tensor(array2)

    # Repeat each element of array1 len(array2) times
    array1_rep = array1.repeat_interleave(len(array2))

    # Repeat array2 len(array1) times
    array2_rep = array2.repeat(len(array1))

    # Combine the arrays
    combined = torch.stack((array1_rep, array2_rep), dim=1)
    return combined



def compute_spearman(a, b):
    # Check if a and b are tensors
    if isinstance(a, torch.Tensor):
        a = a.detach().numpy()
    if isinstance(b, torch.Tensor):
        b = b.detach().numpy()

    coef, p = spearmanr(a, b)
    return coef, p


def feature_selection_every_epoch(emb_all, loc, epoch, name_list = ['SapBERT','CODER','BGE','OPENAI', 'MGB SPPMI','VA SPPMI','UPMC SPPMI','BCH SPPMI','Duke SPPMI', 'MIMIC SPPMI', 'Bor SPPMI', 'GAME'], code_list = ["PheCode:428", "PheCode:296.2", "PheCode:714", "PheCode:290.11", "PheCode:250.1", "PheCode:250.2", "PheCode:555.1", "PheCode:555.2", "PheCode:428.1", 'PheCode:714.1'], RECORD=None, api_key=None, config=None):
    # load data
    emb_all = [pd.DataFrame(emb.cpu().numpy()) for emb in emb_all]
    name_desc = pd.read_csv(f'{config["input_dir"]}/name_desc/unique_name_desc_LP.csv')
    name_all = name_desc.iloc[:,0]
    unique_code = pd.DataFrame(name_all).drop_duplicates().values
    unique_code = np.concatenate(unique_code)
    unique_desc = name_desc.drop_duplicates(subset=name_desc.columns[0]).iloc[:,1]
    unique_desc = unique_desc.values

    name_desc = pd.DataFrame(np.column_stack([unique_code, unique_desc]))
    name_desc.columns = ['sign_id', 'desc']

    # do not contain the replicated desc in same type; for LOINC:LP and their childs, only use once
    name_desc['type_desc'] = pd.Series(ret_type(name_desc['sign_id'])) + ':' + name_desc['desc']
    unique_combinations = name_desc[['type_desc']].drop_duplicates()
    mapping_list = pd.DataFrame(columns=['name', 'indices'])

    # Populate the mapping list
    for _, row in unique_combinations.iterrows():
        type_desc = row['type_desc']
        indices = name_desc[name_desc['type_desc'] == type_desc].index.tolist()
        new_row = pd.DataFrame({'name': [type_desc], 'indices': [indices]})
        mapping_list = pd.concat([mapping_list, new_row], ignore_index=True)

    corr_list = []
    result_list = []
    for code in code_list:
        add_param = f'{loc}'  # Dynamically change the add parameter
        result, corr = all_fea_select(emb_all, code, name_desc, add=add_param, 
                                 name_list = name_list,
                                feat_max=100, mapping_list=mapping_list, api_key=api_key)
        corr_list.append(corr)
        result_list.append(result)
    
    now_corr = np.mean(corr_list)
    if RECORD is not None:
        if now_corr > RECORD:
            for code, result in zip(code_list, result_list):
                plot_all(result, name_list, path = f"{config['path']}/output/{loc}/cos_sim_ans_{code}_{loc}.png")
        
    return now_corr


def feature_selection(emb_all, code, name_all):
    cos_sims = []
    code_indice = np.where(name_all == code)[0]
    emb_all_tensor = torch.tensor(emb_all.values.astype(np.float32))
    
    emb_code_row = emb_all.iloc[code_indice, :].values
    emb_code_tensor = torch.tensor(emb_code_row.astype(np.float32))
    cos_sim = torch.matmul(emb_all_tensor, emb_code_tensor.t()) / torch.norm(emb_code_tensor)**2
    return cos_sim


def all_fea_select(emb_all, code, name_desc, name_list, feat_max=100, add=None, negative=None, edges=None, neg=None, mapping_list=None, api_key=None):
    desc_code = name_desc.iloc[np.where(name_desc.iloc[:,0].values == code)[0],1].values
    name_all = name_desc.iloc[:,0].values
    code_indice = np.where(name_all == code)[0]

    if edges is not None:
        index_edges = np.union1d(name_all[edges[1][np.where((pd.Series(edges[0]).isin(code_indice)).values)[0]]], name_all[edges[0][np.where((pd.Series(edges[1]).isin(code_indice)).values)[0]]])
        index_edges_series = pd.Series(index_edges)
    
    if neg is not None:
        index_neg = np.union1d(name_all[neg[1][np.where((pd.Series(neg[0]).isin(code_indice)).values)[0]]],name_all[neg[0][np.where((pd.Series(neg[1]).isin(code_indice)).values)[0]]])
        index_neg_series = pd.Series(index_neg)

    index_all = []
    cos_all = []

    for emb in emb_all:
        cos = feature_selection(emb, code, name_all)
        cos = cos.reshape(-1)
        cos_all.append(cos)

        if mapping_list is not None:
            cos_list = []
            ind_all_list = []
            for mapping in mapping_list['indices'].values:
                tmp = [cos[i] for i in mapping]
                max_value = max(tmp)
                max_index = np.argmax(tmp)
                cos_list.append(max_value)
                ind_all_list.append(mapping[max_index])
    
            cos = cos_list
    
        top_values, top_indices = torch.topk(torch.tensor(cos), k=feat_max, largest=True, sorted=True)
    
        if mapping_list is not None:
            top_indices = [ind_all_list[i] for i in top_indices.numpy()]
        
        index_all.append(top_indices)

    pos_index = np.concatenate(index_all)
    pos_all = [cos[pos_index] for cos in cos_all]
    
    if negative is None:
        all_index = pos_index
        all_sim_all = cos_all
    else:
        # put into negative list
        nega_list = [np.where(name_all == x)[0] for x in negative.iloc[:,0].values]
        neg_index = np.concatenate(nega_list)
        neg_all = [cos[neg_index] for cos in cos_all]
        all_index = np.concatenate([pos_index, neg_index])
        all_sim_all = [np.concatenate([cos_all[i], neg_all[i]]) for i in range(len(cos_all))]
        
    unique_desc = name_desc.iloc[:,1]
    data = {'index': all_index,
            'name': name_all[all_index],
            'desc': unique_desc.iloc[all_index].values}

    for name, cos_sim in zip(name_list, all_sim_all):
        data[f'{name}_cos'] = np.array(cos_sim)
        data[f'{name}_cos'] = data[f'{name}_cos'][data['index']]
    
    data_df = pd.DataFrame(data)
    data_df['selected_by'] = ''
    if edges is not None:
        data_df['GPT3.5_TRUE'] = pd.Series(name_all[data_df['index']]).isin(index_edges_series)
    if neg is not None:
        data_df['GPT3.5_FALSE'] = pd.Series(name_all[data_df['index']]).isin(index_neg_series)

    
    for i, name in enumerate(name_list):
        indices = np.array(index_all[i])
        data_df.loc[data_df['index'].isin(indices), 'selected_by'] += f'{name}&'
        
    if os.path.exists(f"{config['path']}/supp_code/feature_selection/input/GPT4_{code.replace(':', '_')}.csv"):
        score_ = pd.read_csv(f"{config['path']}/supp_code/feature_selection/input/GPT4_{code.replace(':', '_')}.csv")
    else:
        score_ = None
        
    data_df['selected_by'] = data_df['selected_by'].str.rstrip('&')
    pos = data_df.drop_duplicates(subset=data_df.columns[0]).iloc[:,1:]
    pos['gpt4'] =  write_score(score_, pos)
    ask_gpt4(pos, name=code, desc=desc_code, add=add, api_key=api_key)
    pos = pd.read_csv(f"{config['path']}/supp_code/feature_selection/score_all/GPT4_ans_{code}_{add}.csv", index_col = 0)
    pos_new = change_string_to_float(pos)
    pos_new.to_csv(f"{config['path']}/supp_code/feature_selection/score_all/GPT4_ans_{code}_{add}.csv", index=None)
    update_score(f"{config['path']}/supp_code/feature_selection/score_all/GPT4_ans_{code}_{add}.csv", f"{config['path']}/supp_code/feature_selection/input/GPT4_{code.replace(':','_')}.csv")
    # evaluate
    output_str, output_tmp = print_all(pos_new, name_list, name_all)
    with open(f"{config['path']}/output/{add}/GPT4_rank_ans_{code}.txt", 'a') as f:
        f.write('\n' + output_str)
    print(output_str)
    return pos_new, output_tmp


def write_score(pos_score, pos):
    if pos_score is None:
        desired_values = np.nan
    else:
        index_results = [np.where(pos_score.iloc[:, 0].values == x)[0] for x in pos.iloc[:, 1]]
        desired_values = [pos_score.iloc[idx[0], 1] if len(idx) > 0 else np.nan for idx in index_results]
    return desired_values


def find_indices_by_type(pos, selected_type='sap'):
    pos = pos.reset_index()
    # Check if 'selected_by' is a column in the dataframe
    if 'selected_by' not in pos.columns:
        raise ValueError("The DataFrame does not contain a 'selected_by' column")

    # Create a boolean mask where the type is in the 'selected_by' column after splitting by '&'
    mask = pos['selected_by'].apply(lambda x: selected_type in str(x).split('&'))

    # Get the indices where the mask is True
    indices = pos.index[mask].tolist()

    return indices


def plot_all(pos, name_list, indices_=True, path=None):
    # Generate a list of distinct markers
    markers = itertools.cycle(['o', '^', 's', 'p', '*', 'X', 'D', 'h', 'v', '<', '>']) 

    # Use matplotlib's built-in color cycle
    color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']
    colors = itertools.cycle(color_cycle)
    # Calculate the number of rows and columns for subplots
    n_types = len(name_list)
    n_cols = 4  # You can adjust this to change the layout
    n_rows = n_types // n_cols + (n_types % n_cols > 0)

    # Create a figure and a set of subplots
    fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 10), squeeze=False)

    for i, selected_type in enumerate(name_list):
        marker = next(markers)
        color = next(colors)

        if indices_ is True:
            indices = find_indices_by_type(pos, selected_type)  # Define this function
        else:
            indices = range(pos.shape[0])

        cos_column = f'{selected_type}_cos'
        if cos_column not in pos.columns:
            raise ValueError(f"The DataFrame does not contain a column named '{cos_column}'")

        # Find the appropriate subplot
        ax = axs[i // n_cols, i % n_cols]

        # Scatter plot
        ax.scatter(pos[cos_column].values[indices], pos['gpt4'].values[indices],
                   marker=marker, color=color, label=selected_type)

        # Calculate the median of y values
        quarter_y = np.quantile(pos['gpt4'].values[indices], 0.25)
        quarter3_y = np.quantile(pos['gpt4'].values[indices], 0.75)
        median_y = np.median(pos['gpt4'].values[indices])
        mean_y = np.mean(pos['gpt4'].values[indices])

        # Plot the horizontal line at the median value
        ax.axhline(median_y, color='r', linestyle='--', alpha=0.7, label=f'Median of GPT-4 Values for {selected_type}')
        ax.axhline(quarter_y, color='m', linestyle=':', alpha=0.7, label=f'quantile of GPT-4 Values for {selected_type}')
        ax.axhline(quarter3_y, color='b', linestyle='-', alpha=0.7, label=f'3 quantile of GPT-4 Values for {selected_type}')
        
        # label the text
        print(f'{name_list[i]} -- Median -- {np.round(median_y,3)} -- Mean -- {np.round(mean_y,3)}')
        # ax.text(0.75, 0.7, f'Median: {np.round(median_y,3)}', transform=ax.get_yaxis_transform(), 
        #         horizontalalignment='left', color='red')
        # ax.text(0.75, 0.6, f'Mean: {np.round(mean_y,3)}', transform=ax.get_yaxis_transform(), 
                # horizontalalignment='left', color='green')
        # Labels, legend, and title
        ax.set_xlabel(f'{selected_type.capitalize()} Cos')  # X-axis label for each subplot
        ax.set_ylabel('GPT-4 Values')  # Y-axis label for each subplot
        ax.legend()  # This will add the legend to each subplot
        ax.set_title(f'{selected_type.capitalize()} Cosine Similarity vs GPT-4 Values')  # Title for each subplot

    plt.tight_layout()  
    if path is not None:
        plt.savefig(f"{path}")
    plt.show()
    plt.close()



def print_all(pos, name_list, unique_name):
    output = io.StringIO()
    for i in range(len(name_list)):
        # index = np.where(pos[pos['gpt4'].notna()]['name'].isin(unique_name[config['inst_row'][0]]))[0]
        output_tmp = compute_spearman(np.array(pos[pos['gpt4'].notna()][f'{name_list[i]}_cos']), np.array(pos[pos['gpt4'].notna()]['gpt4']))
        output_tmp = np.round(output_tmp[0],3)
        # output.write(f'{name_list[i]}: {output_tmp} ({len(index)})\t')
        output.write(f'{name_list[i]}: {output_tmp}\t')

    return output.getvalue(), output_tmp


def change_string_to_float(pos_all):
    pos_all_new = pos_all.copy()
    pos_all_new['gpt4'] = pos_all_new['gpt4'].apply(lambda x: float(x) if str(x).replace('.', '', 1).isdigit() else 0.0)
    return pos_all_new


def update_score(path, path_origin=None):
    score_new = pd.read_csv(path)
    score_new = score_new[['desc', 'gpt4']]

    if os.path.exists(path_origin):
        score_ = pd.read_csv(path_origin)
        score_.columns = score_new.columns
        score_new = pd.concat([score_, score_new], axis=0)
        
    score_new.drop_duplicates(subset=score_new.columns[0], keep='first', inplace=True)
    score_new = score_new[pd.to_numeric(score_new.iloc[:,1], errors='coerce').notnull()]
    score_new.to_csv(path_origin, index=None)

    
def ask_gpt4(data, name='PheCode:714.1', desc='Rheumatoid Arthritis', add=None, api_key=None):
    # model_engine = "gpt-4"
    # model_engine = 'gpt-4-turbo-preview'
    # model_engine = "gpt-3.5-turbo"
    model_engine = 'gpt-4o-mini'
    client = OpenAI(api_key=api_key)
    pd.DataFrame(data.columns).transpose().to_csv(f"{config['path']}/supp_code/feature_selection/score_all/GPT4_ans_{name}_{add}.csv", header=None) 
    data.reset_index()
    data = np.asarray(data)
    temp = 0 # run the code if broke
    ans = []
    
    def gen_prompt(i):
        prompt = (f"Is the following medical code related to {desc}?\n\n"
                  f"{data[i,1]}\n\n"
                  f"Please provide a score between 0 and 1 indicating the likelihood that this code is related to {desc}. "
                  f"For instance, a score of 1 indicates complete relevance, whereas a score of 0 indicates no relevance. "
                  f"Provide your score as a decimal (e.g., 0.13, 0.75, etc.) without any additional information or explanations.")
        return prompt
    
    range_ask = np.where(np.isnan(data[:,-1].astype(float)))[0]
    for i in range(temp, data.shape[0]):
        if i not in range_ask:
            pd.DataFrame(data=data[i,:].reshape(-1,1)).transpose().to_csv(f"{config['path']}/supp_code/feature_selection/score_all/GPT4_ans_{name}_{add}.csv", header=False, mode="a")
            ans.append(data[i,-1])
            continue
        response = client.chat.completions.create(
            model=model_engine,
            temperature=0.0,
            messages=[
                {"role": "system",
                "content": f"As a knowledgeable assistant supporting a healthcare professional, your task is to help determine whether various medical codes are related to {desc} based on their provided details. The professional values concise yet precise responses."},
                {"role": "user",
                 "content": gen_prompt(i)}]
        )
        print(gen_prompt(i))
        message = response.choices[0].message.content
        print(message)
        ans.append(message)
        re = np.vstack([data[i,:-1].reshape(-1,1), ans[i - temp]])
        print(i, re)
        df = pd.DataFrame(data=re)
        df = df.transpose()
        df.to_csv(f"{config['path']}/supp_code/feature_selection/score_all/GPT4_ans_{name}_{add}.csv", header=False, mode="a")
        

def sample_and_combine_edges(edge_all_sim, edge_all_rel, config):
    num_edges_to_sample = round(edge_all_rel.size(1) * config['drop_p'])
    permuted_indices = torch.randperm(edge_all_rel.size(1))
    sampled_indices = permuted_indices[:num_edges_to_sample]
    sampled_edges = edge_all_rel[:, sampled_indices]
    edge_index = torch.cat((edge_all_sim, sampled_edges), dim=1)
    return edge_index